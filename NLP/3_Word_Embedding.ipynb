{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word Embedding with practical</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import nltk # natural language toolkit\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import warnings # to ignore warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the sam collection data into pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Assets/spam.csv' , encoding='latin-1' ) # encoding='latin-1' is used to avoid UnicodeDecodeError\n",
    "columns = ['lable', 'message'] # renaming the columns\n",
    "df.columns=columns # assigning the new column names to the data\n",
    "df.head() # checking the first 5 rows of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jayku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importing Data Preprocessing Libraries and Downloading Stopwords\n",
    "\n",
    "import re # regular expression\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "from nltk.stem import PorterStemmer # stemming\n",
    "\n",
    "nltk.download('stopwords') # downloading the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [] # creating an empty list to store the cleaned data\n",
    "\n",
    "for i in range(0, len(df)):  # iterating through the data\n",
    "    review = re.sub(\"[^a-zA-z0-9]\", \" \", df['message'][i]) # removing special characters\n",
    "    review = review.lower() # converting the text to lowercase\n",
    "    review = review.split() # splitting the text into words\n",
    "    ps = PorterStemmer() # creating an object of the PorterStemmer class\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # stemming and removing stopwords\n",
    "    review = \" \".join(review) # joining the words\n",
    "    corpus.append(review) # appending the cleaned data to the list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5] # checking the first 5 cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranning model with BOW (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating the Bag of Words Model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # importing CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500, binary= True, ngram_range=(2,2)) # creating an object of CountVectorizer class\n",
    "X = cv.fit_transform(corpus) # fitting the data to the model and transforming the data\n",
    "X = X.toarray() # converting the data to an array\n",
    "X.shape # checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Splitting the Data into Training and Testing Sets\n",
    "\n",
    "y= pd.get_dummies(df['lable']) # creating dummies of the target variable\n",
    "y = y.iloc[:,1].values # taking the values of the target variable\n",
    "y.shape # checking the shape of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split # importing train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB # importing MultinomialNB\n",
    "\n",
    "## Splitting the Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) \n",
    "\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train) # fitting the model on the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the Model\n",
    "y_pred = spam_detect_model.predict(X_test) # predicting the target variable on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9829596412556054\n",
      "[[946   3]\n",
      " [ 16 150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99       949\n",
      "        True       0.98      0.90      0.94       166\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix # importing confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score # importing classification_report\n",
    "confusion_m = confusion_matrix(y_test, y_pred) # creating the confusion matrix\n",
    "classification = classification_report(y_test, y_pred) # creating the classification report\n",
    "\n",
    "print(accuracy_score(y_test, y_pred)) # printing the accuracy score\n",
    "print(confusion_m) # printing the confusion matrix\n",
    "print(classification) # printing the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating the TF-IDF Model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # importing TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=2500, binary= True, ngram_range=(1,2)) #  creating an object of CountVectorizer class\n",
    "X = tv.fit_transform(corpus) # fitting the data to the model and transforming the data\n",
    "X = X.toarray() # converting the data to an array\n",
    "X.shape # checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split # importing train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB # importing MultinomialNB\n",
    "\n",
    "## Splitting the Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) \n",
    "\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train) # fitting the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the Model\n",
    "y_pred = spam_detect_model.predict(X_test) # predicting the target variable on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9757847533632287\n",
      "[[948   1]\n",
      " [ 26 140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99       949\n",
      "        True       0.99      0.84      0.91       166\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score # importing confusion_matrix\n",
    "from sklearn.metrics import classification_report # importing classification_report\n",
    "confusion_m = confusion_matrix(y_test, y_pred) # creating the confusion matrix\n",
    "classification = classification_report(y_test, y_pred) # creating the classification report\n",
    "\n",
    "print(accuracy_score(y_test, y_pred)) # printing the accuracy score\n",
    "print(confusion_m) # printing the confusion matrix\n",
    "print(classification) # printing the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
