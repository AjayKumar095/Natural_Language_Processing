{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:green;'><center>Text Preprocessing in NLP</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial step in Natural Language Processing (NLP) to prepare raw text data for analysis. It involves cleaning, transforming, and converting text into formats that algorithms can effectively process. In text preprocessing we try to convert the text data into numerical so that machine able to understand the text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key concepts in NLP\n",
    "* Corpus\n",
    "* Tokenization\n",
    "* Stopwords\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* One-Hot Encoding\n",
    "* Bag of Words (BoW)\n",
    "* TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "* N-Grams\n",
    "* Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Corpus\n",
    "What is it?\n",
    " - A corpus is a collection of text data used for analysis or training NLP models. For example, a set of news articles, books, or social media posts can constitute a corpus.\n",
    "\n",
    "When to use?\n",
    " - A corpus is essential in NLP because it provides the raw data for model training or linguistic research.\n",
    "\n",
    "Advantages:\n",
    " - Provides diverse data for training.\n",
    " - Can be tailored for specific tasks (e.g., sentiment analysis, chatbots).\n",
    "\n",
    "Disadvantages:\n",
    " - May require extensive cleaning and annotation.\n",
    " - Large corpora can be computationally expensive to process.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      "  Barnardo. Who's there?\n",
      "  Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      "   Bar. Long liue the King\n",
      "\n",
      "   Fran. Barnardo?\n",
      "  Bar. He\n",
      "\n",
      "   Fran. You come most carefully vpon your houre\n",
      "\n",
      "   Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
      "\n",
      "   Fran. For this releefe much thankes: 'Tis bitter cold,\n",
      "And I am sicke at heart\n",
      "\n",
      "   Barn. Haue you had quiet Guard?\n",
      "  Fran. Not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\jayku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Generating a raw corpus from nltk \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "corpus = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "What is it?\n",
    " - Tokenization is the process of breaking text into smaller pieces, such as words or sentences. These smaller pieces are called \"tokens.\"\n",
    "\n",
    "When to use?\n",
    " - When text needs to be processed word by word or sentence by sentence.\n",
    "Necessary for tasks like word embeddings, sentiment analysis, and more.\n",
    "\n",
    "Advantages:\n",
    " - Simplifies text processing.\n",
    " - Forms the basis for more complex operations (e.g., parsing).\n",
    "\n",
    "Disadvantages:\n",
    " - May struggle with ambiguous cases (e.g., \"New York\" vs. \"New\" and \"York\").\n",
    " - Requires additional steps for handling punctuation or special characters.\n",
    " \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'tokenize', 'this', 'text', '.']\n",
      "Sentences: ['Natural Language Processing is amazing!', \"Let's tokenize this text.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing is amazing! Let's tokenize this text.\"\n",
    "\n",
    "# Tokenize into words\n",
    "words = word_tokenize(text)\n",
    "print(\"Words:\", words)\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above exapmle we see the word tokenizer return a list of all words, and the sentance tokenizer return the list of seprate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stemming\n",
    "What is it?\n",
    " - Stemming reduces words to their root form, often producing non-meaningful results (e.g., \"running\" â†’ \"run\").\n",
    "\n",
    "When to use?\n",
    " - Useful for simple tasks like search engines or spam filters.\n",
    "When accuracy isn't critical.\n",
    "\n",
    "Advantages:\n",
    " - Fast and computationally cheap.\n",
    "\n",
    "Disadvantages:\n",
    " - May not produce meaningful results.\n",
    " - Over-aggressive stemming can lead to loss of information.\n",
    " \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'run', 'easili', 'faster']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"easily\", \"faster\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that some words have no meanings like \"easili\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lemmatization\n",
    "What is it?\n",
    " - Lemmatization reduces words to their dictionary form, considering the word's context and meaning.\n",
    "\n",
    "When to use?\n",
    " - When accuracy and context matter (e.g., chatbots, text classification).\n",
    "\n",
    "Advantages:\n",
    " - Produces meaningful base forms of words.\n",
    " - Handles irregular forms better than stemming.\n",
    "\n",
    "Disadvantages:\n",
    " - Slower than stemming.\n",
    " - Requires a dictionary or lexical database.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jayku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'run', 'easily', 'faster']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"runs\", \"easily\", \"faster\"]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(\"Lemmatized Words:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stopwords\n",
    "What is it?\n",
    " - Stopwords are common words (e.g., \"is\", \"and\", \"the\") that add little value to analysis and are often removed.\n",
    "\n",
    "When to use?\n",
    " - For tasks like text classification, where common words don't contribute much information.\n",
    "\n",
    "Advantages:\n",
    " - Reduces noise in text data.\n",
    " - Simplifies analysis.\n",
    "\n",
    "Disadvantages:\n",
    " - Removing stopwords may lose context in some cases.\n",
    " \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['simple', 'example']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jayku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [\"This\", \"is\", \"a\", \"simple\", \"example\"]\n",
    "filtered = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words:\", filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. One-Hot Encoding\n",
    "What is it?\n",
    " - One-Hot Encoding converts text data into binary vectors. Each unique word is represented as a vector with one 1 and the rest 0s.\n",
    "\n",
    "When to use?\n",
    " - Useful for converting categorical text data into numerical form for machine learning.\n",
    "Commonly used in simple NLP models or as input to embeddings.\n",
    "\n",
    "Advantages:\n",
    " - Easy to implement.\n",
    " - Provides a unique representation for each word.\n",
    "\n",
    "Disadvantages:\n",
    " - Results in high-dimensional vectors for large vocabularies.\n",
    " - Does not capture relationships between words (e.g., \"king\" and \"queen\").\n",
    " \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded:\n",
      "   (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "words = [['hello', 'world', 'hello']]\n",
    "encoder = OneHotEncoder()\n",
    "encoded = encoder.fit_transform(np.array(words))\n",
    "print(\"One-Hot Encoded:\\n\", encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  7. Bag of Words (BoW)\n",
    "What is it?\n",
    " - BoW is a representation where text is converted into a vector of word counts. It ignores the order and grammar of words.\n",
    "\n",
    "When to use?\n",
    " - Suitable for document classification, spam detection, or sentiment analysis.\n",
    "Works well with small datasets and simple models.\n",
    "\n",
    "Advantages:\n",
    " - Easy to understand and implement.\n",
    " - Can handle large text data efficiently.\n",
    "\n",
    "Disadvantages:\n",
    " - Ignores word order and semantics.\n",
    " - Produces sparse matrices with large vocabularies.\n",
    " \n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Features:\n",
      " [[1 0 0 1 1 1]\n",
      " [1 1 1 0 1 0]]\n",
      "Vocabulary:\n",
      " {'this': 5, 'is': 0, 'sample': 3, 'text': 4, 'preprocessing': 2, 'key': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"This is a sample text\", \"Text preprocessing is key\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"BoW Features:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\\n\", vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "What is it?\n",
    " - TF-IDF weighs words based on how frequently they appear in a document (TF) and how unique they are across the corpus (IDF).\n",
    "\n",
    "When to use?\n",
    " - Useful for keyword extraction, document similarity, or search engines.\n",
    " - Better than BoW for capturing word importance.\n",
    "\n",
    "Advantages:\n",
    " - Captures importance of words in context.\n",
    " - Reduces the impact of common words like \"is\" or \"the.\"\n",
    "\n",
    "Disadvantages:\n",
    " - Requires preprocessing to remove stopwords.\n",
    " - Computationally intensive for large corpora.\n",
    "\n",
    "Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features:\n",
      " [[0.40993715 0.         0.         0.57615236 0.40993715 0.57615236]\n",
      " [0.40993715 0.57615236 0.57615236 0.         0.40993715 0.        ]]\n",
      "Vocabulary:\n",
      " {'this': 5, 'is': 0, 'sample': 3, 'text': 4, 'preprocessing': 2, 'key': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"This is a sample text\", \"Text preprocessing is key\"]\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(text)\n",
    "\n",
    "print(\"TF-IDF Features:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\\n\", tfidf.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. N-Grams\n",
    "\n",
    "What is it?\n",
    " - N-Grams are sequences of n words or characters. For example, bigrams are two-word sequences, and trigrams are three-word sequences.\n",
    "\n",
    "When to use?\n",
    " - Helpful for capturing word context and structure in text.\n",
    "Commonly used in machine translation and speech recognition.\n",
    "\n",
    "Advantages:\n",
    " - Captures relationships between words.\n",
    " - Useful for context-sensitive tasks.\n",
    "\n",
    "Disadvantages:\n",
    " - Increases dimensionality with larger n.\n",
    " - May result in sparsity.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-grams: [('Natural', 'Language'), ('Language', 'Processing')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"Natural Language Processing\"\n",
    "# Generate bi-grams\n",
    "bigrams = list(ngrams(text.split(), 2))\n",
    "print(\"Bi-grams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Word2Vec\n",
    "What is it?\n",
    " - Word2Vec is a neural network-based method to generate word embeddings that capture semantic relationships between words.\n",
    "\n",
    "When to use?\n",
    " - For tasks requiring semantic similarity (e.g., recommendation systems, sentiment analysis).\n",
    "\n",
    "Advantages:\n",
    " - Captures relationships between words (e.g., \"king - man + woman = queen\").\n",
    " - Efficient for large vocabularies.\n",
    "\n",
    "Disadvantages:\n",
    " - Requires significant data for training.\n",
    " - Training can be computationally intensive.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'natural': [-0.0960355   0.05007293 -0.08759586 -0.04391825 -0.000351   -0.00296181\n",
      " -0.0766124   0.09614743  0.04982058  0.09233143]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"natural\", \"language\", \"processing\"], [\"text\", \"mining\"]]\n",
    "model = Word2Vec(sentences, vector_size=10, min_count=1, workers=1)\n",
    "\n",
    "# Word vector for \"natural\"\n",
    "print(\"Vector for 'natural':\", model.wv['natural'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
